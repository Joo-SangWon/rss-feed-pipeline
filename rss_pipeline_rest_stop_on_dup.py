import os, json, time
from datetime import datetime
from urllib.parse import urlsplit, urlunsplit, parse_qsl, urlencode

import requests
import feedparser
import pymysql
from rapidfuzz import process, fuzz

# =========================
# ì„¤ì •
# =========================
DB_HOST = "192.168.0.198"
DB_PORT = 3306
DB_USER = "stockAdm"
DB_PASSWORD = "09stockAdm1@"
DB_NAME = "kstock"
DB_CHARSET = "utf8mb4"

OPENAI_API_KEY = os.getenv("OPENAI_API_KEY", "")
OPENAI_MODEL = "gpt-4o-mini"

RSS_FEEDS = {
    'ì—°í•©ë‰´ìŠ¤_ê²½ì œ': 'https://www.yna.co.kr/rss/economy.xml',
    'ì—°í•©ë‰´ìŠ¤_ì‚¬íšŒ': 'https://www.yna.co.kr/rss/society.xml',
    'ë§¤ì¼ê²½ì œ_ê²½ì œ': 'https://www.mk.co.kr/rss/30100041/',
    'ë§¤ì¼ê²½ì œ_ì‚¬íšŒ': 'https://www.mk.co.kr/rss/30100029/',
    'ê²½í–¥ì‹ ë¬¸_ê²½ì œ': 'https://www.khan.co.kr/rss/rssdata/economy_news.xml',
    'ê²½í–¥ì‹ ë¬¸_ì‚¬íšŒ': 'https://www.khan.co.kr/rss/rssdata/society_news.xml',
    'í•œêµ­ê²½ì œ_ë§ˆì¼“': 'https://www.hankyung.com/feed/market',
    'í•œêµ­ê²½ì œ_ì‚¬íšŒ': 'https://www.hankyung.com/feed/society',
    'ë™ì•„ì¼ë³´_ê²½ì œ': 'https://rss.donga.com/economy.xml',
    'ë™ì•„ì¼ë³´_ì‚¬íšŒ': 'https://rss.donga.com/national.xml',
}

# =========================
# DB ìœ í‹¸
# =========================
def get_conn():
    return pymysql.connect(
        host=DB_HOST, port=DB_PORT, user=DB_USER, password=DB_PASSWORD,
        database=DB_NAME, charset=DB_CHARSET, autocommit=True
    )

def create_table(conn):
    """
    ìƒˆë¡œ ë§Œë“œëŠ” í™˜ê²½ì´ë©´ ì•„ë˜ ìŠ¤í‚¤ë§ˆ ì‚¬ìš© (1~5 ì¢…ëª© ì»¬ëŸ¼ í¬í•¨)
    ê¸°ì¡´ í™˜ê²½ì´ë©´ ë‹¤ìŒ ë§ˆì´ê·¸ë ˆì´ì…˜ì„ ë¨¼ì € ì‹¤í–‰í•˜ì„¸ìš”:
      ALTER TABLE rss
        CHANGE stock_code stock_code1 VARCHAR(10) NULL,
        CHANGE stock_name stock_name1 VARCHAR(50) NULL;
      ALTER TABLE rss
        ADD COLUMN stock_code2 VARCHAR(10) NULL AFTER stock_name1,
        ADD COLUMN stock_name2 VARCHAR(50) NULL AFTER stock_code2,
        ADD COLUMN stock_code3 VARCHAR(10) NULL AFTER stock_name2,
        ADD COLUMN stock_name3 VARCHAR(50) NULL AFTER stock_code3,
        ADD COLUMN stock_code4 VARCHAR(10) NULL AFTER stock_name3,
        ADD COLUMN stock_name4 VARCHAR(50) NULL AFTER stock_code4,
        ADD COLUMN stock_code5 VARCHAR(10) NULL AFTER stock_name4,
        ADD COLUMN stock_name5 VARCHAR(50) NULL AFTER stock_code5;
    """
    sql = """
    CREATE TABLE IF NOT EXISTS rss (
        id INT AUTO_INCREMENT PRIMARY KEY,
        source VARCHAR(50),
        title TEXT,
        link VARCHAR(500) UNIQUE,
        pub_date DATETIME,
        summary TEXT,
        stock_code1 VARCHAR(10),
        stock_name1 VARCHAR(50),
        stock_code2 VARCHAR(10),
        stock_name2 VARCHAR(50),
        stock_code3 VARCHAR(10),
        stock_name3 VARCHAR(50),
        stock_code4 VARCHAR(10),
        stock_name4 VARCHAR(50),
        stock_code5 VARCHAR(10),
        stock_name5 VARCHAR(50),
        created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
        INDEX idx_pub_date (pub_date)
    ) CHARACTER SET utf8mb4 COLLATE utf8mb4_unicode_ci;
    """
    with conn.cursor() as cur:
        cur.execute(sql)

# =========================
# ë„ìš°ë¯¸
# =========================
_TRACKING_PARAMS_PREFIX = ("utm_", )
_TRACKING_PARAMS_EXACT = {"gclid", "fbclid", "msclkid", "ref", "referrer"}

def normalize_link(url: str) -> str:
    if not url:
        return url
    parts = urlsplit(url)
    scheme = parts.scheme.lower()
    netloc = parts.netloc.lower()
    fragment = ""
    q = []
    for k, v in parse_qsl(parts.query, keep_blank_values=True):
        if k.startswith(_TRACKING_PARAMS_PREFIX) or k in _TRACKING_PARAMS_EXACT:
            continue
        q.append((k, v))
    query = urlencode(q, doseq=True)
    path = parts.path or "/"
    return urlunsplit((scheme, netloc, path, query, fragment))

def to_datetime_safe(entry):
    try:
        if hasattr(entry, "published_parsed") and entry.published_parsed:
            return datetime(*entry.published_parsed[:6])
    except Exception:
        pass
    return None

# =========================
# stock_master
# =========================
def load_stock_master(conn):
    sql = "SELECT stock_code, stock_name FROM stock_master"
    with conn.cursor() as cur:
        cur.execute(sql)
        rows = cur.fetchall()
    master = []
    for code, name in rows:
        master.append({"code": code, "name": name, "aliases": [name]})
    return master

def build_name_index(master):
    idx = []
    for m in master:
        idx.extend(m["aliases"])
    return idx

# =========================
# LLM (REST)
# =========================
def call_llm_companies_rest(title: str, summary: str, timeout=15) -> list[str]:
    """
    ìµœëŒ€ 5ê°œ ê¸°ì—…ëª…ì„ JSON ë°°ì—´ë¡œë§Œ ë°˜í™˜. í˜•ì‹ ì˜¤ë¥˜ ì‹œ [].
    """
    if not OPENAI_API_KEY:
        raise RuntimeError("OPENAI_API_KEY ë¯¸ì„¤ì •")
    url = "https://api.openai.com/v1/chat/completions"
    headers = {"Authorization": f"Bearer {OPENAI_API_KEY}", "Content-Type": "application/json"}
    prompt = f"""
ë‹¹ì‹ ì€ í•œêµ­ ë‰´ìŠ¤ì—ì„œ ê¸°ì—…ëª…ì„ ì‹ë³„í•˜ëŠ” ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.
ì•„ë˜ ê¸°ì‚¬ ì œëª©ê³¼ ìš”ì•½ì„ ë³´ê³ , ê´€ë ¨ëœ í•œêµ­ ìƒì¥ì‚¬(ê¸°ì—…)ëª…ì„ **ìµœëŒ€ 5ê°œê¹Œì§€** JSON ë°°ì—´ë¡œë§Œ ì¶œë ¥í•˜ì„¸ìš”.
ë¶ˆëª…í™•í•˜ë©´ ë¹ˆ ë°°ì—´([])ì„ ì¶œë ¥í•˜ì„¸ìš”. ë‹¤ë¥¸ ë§ì€ ì“°ì§€ ë§ˆì„¸ìš”.

ì œëª©: {title}
ìš”ì•½: {summary}

ì¶œë ¥ ì˜ˆ(ìˆì„ ë•Œ): ["ì‚¼ì„±ì „ì", "ì¹´ì¹´ì˜¤", "ë„¤ì´ë²„"]
ì¶œë ¥ ì˜ˆ(ì—†ì„ ë•Œ): []
"""
    payload = {"model": OPENAI_MODEL, "messages": [{"role": "user", "content": prompt}], "temperature": 0}
    r = requests.post(url, headers=headers, data=json.dumps(payload), timeout=timeout)
    r.raise_for_status()
    content = r.json()["choices"][0]["message"]["content"].strip()
    try:
        data = json.loads(content)
        return [str(x) for x in data][:5] if isinstance(data, list) else []
    except Exception:
        return []

# =========================
# ë§¤í•‘ (ì—¬ëŸ¬ ì¢…ëª©)
# =========================
def map_name_to_master(company_name: str, name_index, master, threshold: int = 85):
    if not company_name:
        return None
    best = process.extractOne(company_name, name_index, scorer=fuzz.WRatio)
    if not best:
        return None
    match, score, idx = best
    if score < threshold:
        return None
    for m in master:
        if match in m["aliases"]:
            return {"stock_code": m["code"], "stock_name": m["name"], "score": score}
    return None

def select_top_unique_mappings(candidates, name_index, master, limit=5):
    mapped = []
    seen_codes = set()
    for c in candidates:
        res = map_name_to_master(c, name_index, master)
        if not res:
            continue
        code = res["stock_code"]
        if code in seen_codes:
            continue
        seen_codes.add(code)
        mapped.append(res)
    mapped.sort(key=lambda x: x["score"], reverse=True)
    return mapped[:limit]

# =========================
# DB ì¡°ì‘
# =========================
def insert_shell(cur, source, title, link, pub_date, summary):
    """
    ê»ë°ê¸° insert (ë§¤í•‘ ì—†ì´). ì¤‘ë³µì´ë©´ False ë°˜í™˜ â†’ í•´ë‹¹ í”¼ë“œ ì¤‘ë‹¨
    """
    cur.execute(
        """
        INSERT IGNORE INTO rss (
            source, title, link, pub_date, summary,
            stock_code1, stock_name1, stock_code2, stock_name2,
            stock_code3, stock_name3, stock_code4, stock_name4,
            stock_code5, stock_name5
        )
        VALUES (%s, %s, %s, %s, %s, NULL, NULL, NULL, NULL, NULL, NULL, NULL, NULL, NULL, NULL)
        """,
        (source, title, link, pub_date, summary)
    )
    return cur.rowcount > 0

def update_mapping_multi(cur, link, mappings):
    """
    mappings: [{"stock_code":..., "stock_name":...}, ...] ìµœëŒ€ 5ê°œ
    """
    codes = [m["stock_code"] for m in mappings] + [None]*5
    names = [m["stock_name"] for m in mappings] + [None]*5
    codes = codes[:5]; names = names[:5]

    cur.execute(
        """
        UPDATE rss
        SET
          stock_code1=%s, stock_name1=%s,
          stock_code2=%s, stock_name2=%s,
          stock_code3=%s, stock_name3=%s,
          stock_code4=%s, stock_name4=%s,
          stock_code5=%s, stock_name5=%s
        WHERE link=%s
        """,
        (codes[0], names[0], codes[1], names[1], codes[2], names[2],
         codes[3], names[3], codes[4], names[4], link)
    )

# =========================
# íŒŒì´í”„ë¼ì¸ (ì¤‘ë³µ ê°ì§€ ì‹œ í•´ë‹¹ í”¼ë“œ ì¤‘ë‹¨)
# =========================
def process_feed(conn, master, name_index):
    stats = {
        "feeds_total": len(RSS_FEEDS),
        "feeds_stopped_on_dup": 0,
        "articles_seen": 0,
        "inserted": 0,
        "mapped_articles": 0,      # 1ê°œ ì´ìƒ ë§¤í•‘ëœ ê¸°ì‚¬ ìˆ˜
        "mapped_pairs": 0,         # ì´ ë§¤í•‘ ê±´ìˆ˜(ê¸°ì‚¬Ã—ì¢…ëª©)
        "saved_unmapped": 0,
        "llm_fail": 0,
    }

    with conn.cursor() as cur:
        for name, url in RSS_FEEDS.items():
            print(f"\nğŸ“¡ {name} ìˆ˜ì§‘ ì¤‘... ({url})")
            feed = feedparser.parse(url)

            if getattr(feed, "bozo", False):
                print(f"âš ï¸ íŒŒì‹± ê²½ê³ : {getattr(feed, 'bozo_exception', '')}")

            stop_this_feed = False

            for entry in feed.entries:
                if stop_this_feed:
                    break

                title   = entry.get('title', '[ì œëª© ì—†ìŒ]')
                link    = normalize_link(entry.get('link', ''))
                pubdate = to_datetime_safe(entry)
                summary = entry.get('summary', '') or entry.get('description', '')

                stats["articles_seen"] += 1

                # 0) ì €ì¥ ë¨¼ì € (ì¤‘ë³µì´ë©´ í”¼ë“œ ì¤‘ë‹¨)
                inserted = insert_shell(cur, name, title, link, pubdate, summary)
                if not inserted:
                    print(f"â¹ ì¤‘ë³µ ê°ì§€ â†’ í”¼ë“œ ì¤‘ë‹¨ | {title[:60]}")
                    stats["feeds_stopped_on_dup"] += 1
                    break

                stats["inserted"] += 1

                # 1) ì‹ ê·œë§Œ LLM ë§¤í•‘ (ì—¬ëŸ¬ ì¢…ëª©)
                try:
                    cands = call_llm_companies_rest(title or "", summary or "")
                except Exception as e:
                    print(f"âœ… ì €ì¥(LLM ì‹¤íŒ¨, ë¯¸ë§¤í•‘) | {title[:60]} | ì˜¤ë¥˜: {e}")
                    stats["llm_fail"] += 1
                    stats["saved_unmapped"] += 1
                    continue

                if not cands:
                    print(f"âœ… ì €ì¥(ë¯¸ë§¤í•‘) | {title[:60]} | í›„ë³´ ì—†ìŒ")
                    stats["saved_unmapped"] += 1
                    continue

                mappings = select_top_unique_mappings(cands, name_index, master, limit=5)
                if not mappings:
                    print(f"âœ… ì €ì¥(ë¯¸ë§¤í•‘) | {title[:60]} | í›„ë³´: {cands}")
                    stats["saved_unmapped"] += 1
                    continue

                update_mapping_multi(cur, link, mappings)
                stats["mapped_articles"] += 1
                stats["mapped_pairs"] += len(mappings)

                tags = ", ".join([f"{m['stock_name']}({m['stock_code']})" for m in mappings])
                print(f"âœ… ì €ì¥+ë§¤í•‘ [{tags}] | {title[:60]}")
                time.sleep(0.1)  # ê³¼ë„í˜¸ì¶œ ë°©ì§€(ì˜µì…˜)

    return stats

# =========================
# ë©”ì¸ (ìš”ì•½ ì¶œë ¥)
# =========================
def main():
    conn = get_conn()
    try:
        create_table(conn)  # ìƒˆ í™˜ê²½ì´ë©´ í…Œì´ë¸” ìƒì„±; ê¸°ì¡´ì€ ALTER ì´í›„ì—ë„ ì•ˆì „
        master = load_stock_master(conn)
        name_index = build_name_index(master)
        stats = process_feed(conn, master, name_index)

        print("\n==== ì‹¤í–‰ ìš”ì•½ ====")
        print(f"â€¢ í”¼ë“œ ì´ìˆ˜: {stats['feeds_total']}")
        print(f"â€¢ ì¤‘ë³µìœ¼ë¡œ ì¤‘ë‹¨ëœ í”¼ë“œ: {stats['feeds_stopped_on_dup']}")
        print(f"â€¢ ë³¸ ê¸°ì‚¬ìˆ˜(í”¼ë“œ ë‚´): {stats['articles_seen']}")
        print(f"â€¢ ì‹ ê·œ ì €ì¥: {stats['inserted']}")
        print(f"â€¢ ë§¤í•‘ëœ ê¸°ì‚¬ ìˆ˜: {stats['mapped_articles']}")
        print(f"â€¢ ì´ ë§¤í•‘ ê±´ìˆ˜(ê¸°ì‚¬Ã—ì¢…ëª©): {stats['mapped_pairs']}")
        print(f"â€¢ ë¯¸ë§¤í•‘ ì €ì¥: {stats['saved_unmapped']} (LLM ì‹¤íŒ¨ {stats['llm_fail']} í¬í•¨)")
        print("====================\n")

        return stats
    finally:
        conn.close()

if __name__ == "__main__":
    main()
